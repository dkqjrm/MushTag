{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7_xhlhJicnXu"},"outputs":[],"source":["import torch\n","import time\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import ViltProcessor, ViltForImagesAndTextClassification, ViltConfig, ViltModel, AdamW\n","import requests\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn as nn\n","import ast\n","import shutil\n","import torch.nn.functional as F\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fepmZ_prlV0T"},"outputs":[],"source":["np.random.seed(97)\n","torch.random.manual_seed(97)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoJDjvQb5ddm"},"outputs":[],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKGN_mHAAbuR"},"outputs":[],"source":["def scoring(pred, target, topk):\n","    pred = torch.argsort(pred, dim=1, descending=True)\n","    pred = pred.cpu().detach().numpy()  # [batch_size, hashtag_vocab_size]\n","    target = target  # [batch_size, hashtag_vocab_size]\n","    tag_label = []\n","    for this_data in target:\n","        tag_label.append([])\n","        for idx, each_tag in enumerate(this_data):\n","            if each_tag != 0:\n","                tag_label[-1].append(idx)\n","    precision = []\n","    recall = []\n","    f1 = []\n","    print(pred)\n","    for i in range(len(pred)):\n","        this_precision = 0\n","        this_recall = 0\n","        this_f1 = 0\n","        if (len(tag_label[i]) != 0):\n","            for j in range(topk):\n","                if pred[i][j] in tag_label[i]:\n","                    this_precision += 1\n","            for j in range(len(tag_label[i])):\n","                if tag_label[i][j] in pred[i][:topk]:\n","                    this_recall += 1\n","            this_precision /= topk\n","            this_recall /= len(tag_label[i])\n","            if this_precision != 0 and this_recall != 0:\n","                this_f1 = 2 * (this_precision * this_recall) / (this_precision + this_recall)\n","        precision.append(this_precision)\n","        recall.append(this_recall)\n","        f1.append(this_f1)\n","    return precision, recall, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkwiWRzQfbST"},"outputs":[],"source":["device = torch.device('cuda:0')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFVn1zmPlV0V"},"outputs":[],"source":["class ClassificationModel(nn.Module):\n","    def __init__(self, pretrained_model='dandelin/vilt-b32-mlm'):\n","        super(ClassificationModel, self).__init__()\n","        self.vilt = ViltModel.from_pretrained(pretrained_model)\n","        self.linear = nn.Linear(768,1000)\n","        self.norm = nn.LayerNorm(1000)\n","        self.acti = nn.GELU()\n","        self.linear2= nn.Linear(1000,2000)\n","    \n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        pixel_values=None,\n","        pixel_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        image_embeds=None,\n","        image_token_type_idx=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        pooler_output = self.vilt(input_ids, token_type_ids, attention_mask, pixel_values, pixel_mask).pooler_output\n","        predict = self.linear(pooler_output)\n","        predict = self.norm(predict)\n","        predict = self.acti(predict)\n","        predict = self.linear2(predict)\n","        return predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wnul26fPlV0W"},"outputs":[],"source":["model = ClassificationModel()\n","processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0v6ngnjlV0X"},"outputs":[],"source":["#개별 softmax사용시 location O 수정중입니다 **\n","class SSTDataset(Dataset):\n","    def __init__(self, csv_file, root_dir):\n","        self.df = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.root_dir,str(self.df.iloc[idx]['image_index']) + '.jpg')\n","        image = Image.open(img_name)\n","        text = str(self.df.iloc[idx]['concat_category_location_sen'])\n","        label = ast.literal_eval(self.df.iloc[idx]['new_hashtags_2000_onehot'])\n","        labels = ast.literal_eval(self.df.iloc[idx]['new_hashtags_2000_onehots'])\n","        process_output = self.processor(image, text,truncation=True, padding = 'max_length', return_tensors=\"pt\")\n","        for k,v in process_output.items():\n","            process_output[k] = v.squeeze()\n","        process_output['labels'] = labels\n","        process_output['label'] = label\n","        \n","        return process_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhI88xkclV0X"},"outputs":[],"source":["def collate_fn(batch):\n","    input_ids = [item['input_ids'] for item in batch]\n","    pixel_values = [item['pixel_values'] for item in batch]\n","    attention_mask = [item['attention_mask'] for item in batch]\n","    token_type_ids = [item['token_type_ids'] for item in batch]\n","    labels = [item['labels'] for item in batch]\n","    label = [item['label'] for item in batch]\n","\n","    # create padded pixel values and corresponding pixel mask\n","    encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n","\n","    # create new batch\n","    batch = {}\n","    batch['input_ids'] = torch.stack(input_ids)\n","    batch['attention_mask'] = torch.stack(attention_mask)\n","    batch['token_type_ids'] = torch.stack(token_type_ids)\n","    batch['pixel_values'] = encoding['pixel_values']\n","    batch['pixel_mask'] = encoding['pixel_mask']\n","    batch['labels'] = torch.LongTensor(labels)\n","    batch['label'] = torch.LongTensor(label)\n","\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaJCCTHMlV0X"},"outputs":[],"source":["# 개별 softmax 사용시\n","batch_size = 128\n","dataset = SSTDataset('post_dataset.csv','./image')\n","train_size = int(0.8 * len(dataset))\n","test_val_size = len(dataset) - train_size\n","train_dataset, test_val_dataset = torch.utils.data.random_split(dataset, [train_size, test_val_size])\n","val_size = int(0.5 * len(test_val_dataset))\n","test_size = len(test_val_dataset) - val_size\n","val_dataset, test_dataset = torch.utils.data.random_split(test_val_dataset, [val_size,test_size])\n","train_loader = DataLoader(train_dataset, batch_size = batch_size,collate_fn=collate_fn, shuffle = True)\n","val_loader = DataLoader(val_dataset, batch_size = batch_size, collate_fn=collate_fn, shuffle = True)\n","test_loader = DataLoader(test_dataset, batch_size = batch_size,collate_fn=collate_fn, shuffle = True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsim5y55lV0X"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfwxF60CYEk4"},"outputs":[],"source":["def save_checkpoint(state, is_best, model_save_path, filename):\n","    torch.save(state, filename)\n","    if is_best:\n","        shutil.copyfile(filename, os.path.join(model_save_path, 'model_best.pth.tar'))\n","\n","save_path = './saved_model/'\n","save_path = os.path.join(save_path,\n","                             time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime(time.time())))\n","writer = SummaryWriter(log_dir=save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"517tr21glV0Y"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()\n","#개별 softmax 사용시"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKZw7EOt4vg5"},"outputs":[],"source":["# \n","lr = 0.0001\n","weight_decay = 0.01\n","optimizer = AdamW(model.parameters(), lr=lr, weight_decay = weight_decay) # 대충 옵티마이저 정해야함.\n","threshold = 5\n","topk = 5\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exK4o-ZqlV0Z"},"outputs":[],"source":["with open(os.path.join(save_path,\"config.txt\"),\"w\") as f:\n","    f.write(\"batch_size = \" + str(batch_size)+\"\\n\")\n","    f.write(\"learning_rate = \" + str(lr)+ \"\\n\")\n","    f.write(\"threshold = \" + str(threshold)+ \"\\n\")\n","    f.write(\"weight_decay = \" + str(weight_decay)+ \"\\n\")\n","    f.write(\"optim = \" + str(optimizer) +\"\\n\")\n","    f.write(\"topk = \" + str(topk)+ \"\\n\")\n","    f.write(\"Location = False \" + \"\\n\")\n","    f.write(\"hashtag = 2000 \\n\")\n","    f.write(str(criterion)+'\\n')\n","    f.write(\"drop_index\\n\")\n","    f.write(\"freeze Parameter = False\\n\")\n","    f.write(\"forsoftmax사용\\n\")\n","    f.write(\"Model 1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFbJOBNwZB2w"},"outputs":[],"source":["global_steps = 0\n","epoch = 0\n","max_f1 = 0\n","stop_cnt = 0\n","while True:\n","    epoch += 1\n","    model.train()\n","    precision = []\n","    recall = []\n","    f1 = []\n","    train_loss = 0\n","    cnt = 0\n","    for batch in tqdm(train_loader, total=len(train_loader)):\n","        batch = {k:v.to(device) for k,v in batch.items()}\n","        logits = model(batch['input_ids'],batch['attention_mask'],batch['token_type_ids'],batch['pixel_values'],batch['pixel_mask'])\n","        label = batch['label'].to(device)\n","        labels = batch['labels'].to(device)\n","        print(logits)\n","        loss = criterion(logits,label.float())\n","        train_loss += loss.item()\n","        \n","        cnt += len(label)\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        batch_p, batch_r, batch_f1 = scoring(logits, labels, topk) # topk\n","        precision.extend(batch_p)\n","        recall.extend(batch_r)\n","        f1.extend(batch_f1)\n","\n","        writer.add_scalar(tag='batch_precision',\n","                                scalar_value=sum(batch_p)/len(batch_p),\n","                                global_step=global_steps)\n","        writer.add_scalar(tag='batch_recall',\n","                            scalar_value=sum(batch_r) / len(batch_r),\n","                            global_step=global_steps)\n","        writer.add_scalar(tag='batch_f1',\n","                            scalar_value=sum(batch_f1) / len(batch_f1),\n","                            global_step=global_steps)\n","        writer.add_scalar(tag='batch_loss',\n","                            scalar_value=loss.item(),\n","                            global_step=global_steps)\n","        global_steps += 1\n","\n","    writer.add_scalar(tag='train_precision',\n","                            scalar_value=sum(precision) / len(precision),\n","                            global_step=epoch)\n","    writer.add_scalar(tag='train_recall',\n","                        scalar_value=sum(recall) / len(recall),\n","                        global_step=epoch)\n","    writer.add_scalar(tag='train_f1',\n","                        scalar_value=sum(f1) / len(f1),\n","                        global_step=epoch)\n","    writer.add_scalar(tag='train_loss',\n","                        scalar_value=train_loss / cnt,\n","                        global_step=epoch)\n","    scheduler.step()\n","    \n","    model.eval()\n","    precision = []\n","    recall = []\n","    f1 = []\n","    val_loss = 0\n","    cnt = 0\n","    for batch in tqdm(val_loader, total=len(val_loader)):\n","        batch = {k:v.to(device) for k,v in batch.items()}\n","        label = batch['label'].to(device)\n","        labels = batch['labels'].to(device)\n","        with torch.no_grad():\n","            logits = model(batch['input_ids'],batch['attention_mask'],batch['token_type_ids'],batch['pixel_values'],batch['pixel_mask'])\n","        print(logits)\n","        loss = criterion(logits,label.float())\n","        val_loss += loss.item()\n","        cnt += len(label)\n","        batch_p, batch_r, batch_f1 = scoring(logits, labels, topk) #topk\n","        precision.extend(batch_p)\n","        recall.extend(batch_r)\n","        f1.extend(batch_f1)\n","    val_p = sum(precision)/len(precision)\n","    val_r = sum(recall) / len(recall)\n","    val_f1 = sum(f1) / len(f1)\n","    writer.add_scalar(tag='val_precision',\n","                        scalar_value=val_p,\n","                        global_step=epoch)\n","    writer.add_scalar(tag='val_recall',\n","                        scalar_value=val_r,\n","                        global_step=epoch)\n","    writer.add_scalar(tag='val_f1',\n","                        scalar_value=val_f1,\n","                        global_step=epoch)\n","    writer.add_scalar(tag='val_loss',\n","                        scalar_value=val_loss / cnt,\n","                        global_step=epoch)\n","\n","    if val_f1 > max_f1:\n","        max_f1 = val_f1\n","        stop_cnt = 0\n","        is_best = True\n","    else:\n","        stop_cnt += 1\n","        is_best = False\n","\n","    save_checkpoint({\n","        'epoch': epoch,\n","        'model': model,\n","        'state_dict': model.state_dict(),\n","        'precision': val_p,\n","        'recall': val_r,\n","        'f1-score': val_f1,\n","        'optimizer': optimizer.state_dict()\n","    }, is_best, save_path, os.path.join(save_path, 'epoch' + str(epoch) + '.pth.tar'))\n","\n","    if stop_cnt > threshold: # threshold\n","        print(\"Training finished.\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyUbQKgOlV0h"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"MurshTag.ipynb","provenance":[{"file_id":"1j1wZ2eUa-D_vhrKHOtMbjrmBAgLtzfK_","timestamp":1652099425990}]},"kernelspec":{"display_name":"torch1.12.0-py3.9-cuda11.6","language":"python","name":"torch1.12.0-py3.9-cuda11.6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}